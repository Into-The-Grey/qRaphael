{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_quantized_model.ipynb\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "# Define constants\n",
    "MODEL_ID = \"google/Gemma-2-2b-it\"  # The model you are using\n",
    "MODEL_SAVE_DIR = \"/home/ncacord/qRaphael/models/qRaphael-2b-it\"\n",
    "LOG_DIR = \"/home/ncacord/qRaphael/logs\"\n",
    "CACHE_DIR = \"./cache\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Paths for logging\n",
    "STANDARD_LOG_PATH = os.path.join(LOG_DIR, \"standard/model_init_logs.log\")\n",
    "TRACE_LOG_PATH = os.path.join(LOG_DIR, \"trace/trace_model_init_logs.log\")\n",
    "\n",
    "# Logging configuration\n",
    "log_level = logging.INFO\n",
    "log_file = STANDARD_LOG_PATH\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=log_level,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_file), logging.StreamHandler()],\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def load_and_quantize_model(model_id, quantization_config):\n",
    "    \"\"\"\n",
    "    Load and quantize the model using the provided configuration.\n",
    "\n",
    "    Args:\n",
    "    - model_id (str): The model identifier.\n",
    "    - quantization_config (BitsAndBytesConfig): The quantization configuration.\n",
    "\n",
    "    Returns:\n",
    "    - tokenizer: The tokenizer associated with the model.\n",
    "    - model: The quantized model.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    logger.info(\"Loading and quantizing model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # Automatically map model across available GPUs\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def save_model_and_tokenizer(model, tokenizer, save_directory):\n",
    "    \"\"\"\n",
    "    Save the model and tokenizer to the specified directory.\n",
    "\n",
    "    Args:\n",
    "    - model: The model to save.\n",
    "    - tokenizer: The tokenizer to save.\n",
    "    - save_directory (str): The directory to save the model and tokenizer.\n",
    "    \"\"\"\n",
    "    logger.info(\"Saving model and tokenizer...\")\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    logger.info(f\"Model and tokenizer saved to {save_directory}\")\n",
    "\n",
    "\n",
    "def log_telemetry():\n",
    "    \"\"\"\n",
    "    Log GPU telemetry using nvidia-smi.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running nvidia-smi for telemetry...\")\n",
    "    result = subprocess.run([\"nvidia-smi\"], stdout=subprocess.PIPE)\n",
    "    logger.info(result.stdout.decode(\"utf-8\"))\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load, quantize, and save the model, and log telemetry.\n",
    "    \"\"\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    with tqdm(total=100, desc=\"Loading and quantizing model\") as pbar:\n",
    "        tokenizer, model = load_and_quantize_model(MODEL_ID, quantization_config)\n",
    "        pbar.update(50)\n",
    "\n",
    "        save_model_and_tokenizer(model, tokenizer, MODEL_SAVE_DIR)\n",
    "        pbar.update(50)\n",
    "\n",
    "    log_telemetry()\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"Script completed successfully.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
